{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7437227f-6ecf-4a32-a6fa-9a69b744eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries \n",
    "\n",
    "import json\n",
    "from time import sleep\n",
    "import os\n",
    "import pandas as pd\n",
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer\n",
    "from kafka.admin import NewTopic\n",
    "import argparse\n",
    "import feast\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from feast import FeatureStore\n",
    "from feast.data_source import PushMode\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2fdd5c3-f7dd-4925-86b3-4209d84575ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/jovyan/work/workspace/feature_repo\n"
     ]
    }
   ],
   "source": [
    "os.chdir('./feature_repo')  # Change to the directory where the feature repo is located\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0614e3c-8d07-4ebe-bdb5-11fd52693c09",
   "metadata": {},
   "source": [
    "# What is Feast?\n",
    "\n",
    "Feast (Feature Store) is an open-source feature store designed for machine learning operations. It serves as a centralized repository for managing, storing, and serving machine learning features in both offline (batch) and online (real-time) environments.\n",
    "\n",
    "## Why Feature Stores Matter\n",
    "\n",
    "The feature store acts as a bridge between data engineering and machine learning workflows, addressing key challenges in the ML lifecycle:\n",
    "\n",
    "- **Consistency between training and serving**: Ensures the same feature values are used for both model training and inference.\n",
    "- **Feature reuse**: Allows teams to share and discover features across projects.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. Data Sources\n",
    "- **Batch Source**: Historical data used for training models, typically stored in files (like Parquet) or databases.\n",
    "- **Stream Source**: Real-time data, often coming from event streams or message queues.\n",
    "\n",
    "### 2. Data Storage\n",
    "- **Offline Data Store**: is an interface for working with historical time-series feature values that are stored in data sources. The OfflineStore interface has several different implementations, such as the BigQueryOfflineStore, each of which is backed by a different storage and compute engine.\n",
    "- **Online Data Store**: Stores the latest feature values for real-time serving, typically in low-latency databases like Redis.\n",
    "\n",
    "### 3. Registry\n",
    "The Registry is a database that:\n",
    "- Stores all feature definitions, data sources, and entity relationships (More about this next)\n",
    "- Acts as the source of truth for the entire feature store infrastructure\n",
    "- Tracks feature metadata including descriptions, owners, and data types\n",
    "\n",
    "In its simplest form, it is stored as a file.\n",
    "\n",
    "### 4. Consumers (Data Scientists / ML Engineers)\n",
    "- **Model Training**: Uses historical feature values from the offline store.\n",
    "- **Inference**: Uses real-time feature values from the online store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ab2aa-c363-413c-b415-b8232b349996",
   "metadata": {},
   "source": [
    "## Before we proceed, take a look at the data that we will be using for demonstrating Feast\n",
    "The dataset contains flight information for September 2024, with each row representing a single flight. Key features include:\n",
    "\n",
    "    Flight identifiers: flight_ID (airline code and flight number), origin/destination airports\n",
    "    Schedule information: FlightDate, Distance, CRSElapsedTime (scheduled duration)\n",
    "    Temporal features: DayOfWeek, Month, Quarter\n",
    "    Delay metrics: DepDelay (departure delay), ArrDelay (arrival delay), and categorized delays like WeatherDelay and NASDelay\n",
    "    Computed features: is_holiday, days_to_nearest_holiday, Route (combined origin-destination), and route-based delay statistics\n",
    "\n",
    "The computed features are important in the context of stream transformations. We will take a look at these later on.\n",
    "\n",
    "This data is ideal for our flight delay prediction service as it contains both historical performance metrics and contextual features that can help predict future delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f3ba09-53e5-49e5-ae76-84f7c7b127f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of        flight_ID FlightDate Origin Dest  Distance  CRSElapsedTime  DayOfWeek  \\\n",
      "0        WN_3609 2024-09-01    ABQ  AUS     619.0           105.0          7   \n",
      "402849    AS_500 2024-09-01    SEA  IND    1866.0           247.0          7   \n",
      "402850    AS_502 2024-09-01    SEA  AUS    1770.0           252.0          7   \n",
      "402851    AS_505 2024-09-01    AUS  SEA    1770.0           269.0          7   \n",
      "402852    AS_508 2024-09-01    SEA  CVG    1965.0           265.0          7   \n",
      "...          ...        ...    ...  ...       ...             ...        ...   \n",
      "470381   DL_2851 2024-09-30    SEA  DEN    1024.0           154.0          1   \n",
      "228957   UA_1521 2024-09-30    IAD  MSY     955.0           163.0          1   \n",
      "228958   UA_1520 2024-09-30    MCO  DEN    1546.0           237.0          1   \n",
      "228954   UA_1523 2024-09-30    IAH  PDX    1825.0           269.0          1   \n",
      "229208   UA_1276 2024-09-30    IAH  EWR    1400.0           215.0          1   \n",
      "\n",
      "        Month  Quarter  DepDelay  ...  WeatherDelay  NASDelay  SecurityDelay  \\\n",
      "0           9        3       2.0  ...           NaN       NaN            NaN   \n",
      "402849      9        3       4.0  ...           NaN       NaN            NaN   \n",
      "402850      9        3      -9.0  ...           NaN       NaN            NaN   \n",
      "402851      9        3      68.0  ...           0.0       3.0            0.0   \n",
      "402852      9        3      -7.0  ...           NaN       NaN            NaN   \n",
      "...       ...      ...       ...  ...           ...       ...            ...   \n",
      "470381      9        3     -11.0  ...           NaN       NaN            NaN   \n",
      "228957      9        3       3.0  ...           NaN       NaN            NaN   \n",
      "228958      9        3      -5.0  ...           NaN       NaN            NaN   \n",
      "228954      9        3      -5.0  ...           NaN       NaN            NaN   \n",
      "229208      9        3       6.0  ...           NaN       NaN            NaN   \n",
      "\n",
      "        LateAircraftDelay  ArrDelay  is_holiday  days_to_nearest_holiday  \\\n",
      "0                     NaN      -4.0           0                        1   \n",
      "402849                NaN      -3.0           0                        1   \n",
      "402850                NaN     -11.0           0                        1   \n",
      "402851               68.0      71.0           0                        1   \n",
      "402852                NaN     -15.0           0                        1   \n",
      "...                   ...       ...         ...                      ...   \n",
      "470381                NaN       0.0           0                       14   \n",
      "228957                NaN      10.0           0                       14   \n",
      "228958                NaN     -29.0           0                       14   \n",
      "228954                NaN      -6.0           0                       14   \n",
      "229208                NaN     -10.0           0                       14   \n",
      "\n",
      "          Route route_avg_delay_24h  route_max_delay_24h  \n",
      "0       ABQ_AUS                 NaN                  NaN  \n",
      "402849  SEA_IND                 NaN                  NaN  \n",
      "402850  SEA_AUS                 NaN                  NaN  \n",
      "402851  AUS_SEA                 NaN                  NaN  \n",
      "402852  SEA_CVG                 NaN                  NaN  \n",
      "...         ...                 ...                  ...  \n",
      "470381  SEA_DEN            6.904762                 92.0  \n",
      "228957  IAD_MSY           -4.500000                 -3.0  \n",
      "228958  MCO_DEN            1.272727                 30.0  \n",
      "228954  IAH_PDX           -6.000000                 -5.0  \n",
      "229208  IAH_EWR           29.909090                157.0  \n",
      "\n",
      "[582622 rows x 21 columns]>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('./data/flights_v1.parquet')\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b53350-489e-4114-911c-f65bc6e22853",
   "metadata": {},
   "source": [
    "# Data Sources in Feast\n",
    "\n",
    "Data sources are foundational components in Feast that define where and how feature data is obtained. They serve as the connection points between your raw data and the feature store infrastructure.\n",
    "\n",
    "\n",
    "## Types of Data Sources\n",
    "\n",
    "### 1. FileSource\n",
    "FileSource is used for batch processing scenarios and reads data from files like Parquet, CSV, or other file formats. This is particularly convenient for local development or when working with historical data stored in files.\n",
    "\n",
    "### 2. PushSource\n",
    "PushSource allows feature values to be pushed to the online store and offline store in real-time, making fresh feature values immediately available to applications. This approach enables you to update your feature values on-demand without needing to run a complete batch ingestion cycle.\n",
    "\n",
    "### 3. KafkaSource / Streaming Sources [Still in Alpha]\n",
    "Kafka source connects to Kafka topics for real-time data streaming. It requires a batch source to be specified, which can be used for retrieving historical features. This type of source is essential for online feature serving with real-time updates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3500e-6649-4fed-8efa-367521fa24b7",
   "metadata": {},
   "source": [
    "# Our Data Source Setup\n",
    "\n",
    "In our project, we use two complementary data source types in our feature store implementation. You can examine the detailed setup in the `data_sources.py` file located in the `workspace/feature_repo` folder.\n",
    "\n",
    "## File Source Configuration\n",
    "\n",
    "Our primary data source is configured to read flight data from a Parquet file:\n",
    "\n",
    "```python\n",
    "flight_stats_source = FileSource(\n",
    "    path=\"data/flights_v1.parquet\", \n",
    "    timestamp_field=\"FlightDate\",\n",
    "    file_format=ParquetFormat()\n",
    ")\n",
    "```\n",
    "\n",
    "This configuration uses a local file path for development. The `timestamp_field` parameter indicates that the \"FlightDate\" column contains the event timestamps, which is crucial for Feast to maintain point-in-time correctness when retrieving features.\n",
    "\n",
    "## Alternative S3 Configuration (Commented Out) \n",
    "\n",
    "We also have a commented-out S3 configuration that demonstrates how to connect to data stored in MinIO (an S3-compatible object storage):\n",
    "\n",
    "```python\n",
    "bucket_name = \"feast-bucket\"\n",
    "file_name = \"flights_v1.parquet\"\n",
    "s3_endpoint = \"https://localhost:9000\" \n",
    "\n",
    "flight_stats_source = FileSource(\n",
    "    path=f\"s3://{bucket_name}/{file_name}\",  \n",
    "    timestamp_field=\"FlightDate\",\n",
    "    file_format=ParquetFormat(),\n",
    "    s3_endpoint_override=\"http://localhost:9000\"\n",
    ")\n",
    "```\n",
    "\n",
    "This configuration is currently commented out due to an ongoing bug with S3 endpoint handling in Feast. This will work once the fix is released (https://github.com/feast-dev/feast/pull/5208)\n",
    "\n",
    "## Push Source for Real-time Updates\n",
    "\n",
    "To enable real-time updates to our feature store, we've configured a push source:\n",
    "\n",
    "```python\n",
    "flight_stats_push_source = PushSource(\n",
    "    name=\"flight_stats_push_source\",\n",
    "    batch_source=flight_stats_source,\n",
    ")\n",
    "```\n",
    "\n",
    "This push source:\n",
    "- References our batch source (flight_stats_source) to maintain schema consistency\n",
    "- Allows us to programmatically push new feature values using `store.push()` API calls\n",
    "- Enables real-time feature updates without waiting for batch processing cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73394306-381b-44d0-afdd-f3feecd52220",
   "metadata": {},
   "source": [
    "# Entities in Feast\n",
    "\n",
    "## What are Entities?\n",
    "\n",
    "Entities in Feast are the objects around which features are organized. They serve as primary keys that uniquely identify records in your feature data.\n",
    "You can take a look at the `entities.py` file. It is in same folder of our feast artifact.\n",
    "\n",
    "## Flight Entity\n",
    "\n",
    "```python\n",
    "from feast import Entity\n",
    "\n",
    "# Define an entity for the flight\n",
    "flight = Entity(\n",
    "    name=\"flight\",\n",
    "    join_keys=[\"flight_ID\"],\n",
    "    description=\"Flight identifier\"\n",
    ")\n",
    "```\n",
    "\n",
    "In our flight delay prediction service, this entity definition:\n",
    "- Names \"flight\" as our primary object\n",
    "- Uses \"flight_ID\" as the join key for connecting data across sources\n",
    "- Provides a clear description of its purpose\n",
    "\n",
    "This entity will allow Feast to organize flight-related features (like aircraft data, weather conditions, and historical delays) and enable accurate feature retrieval for both model training and real-time prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40333087-4c56-4fc4-8c6b-edfbbb34af39",
   "metadata": {},
   "source": [
    "# Feature Views in Feast\n",
    "\n",
    "## What are Feature Views?\n",
    "\n",
    "Feature views are a core component in Feast that define a collection of features and their data source. They specify the schema, time-to-live (TTL), and storage locations for features, connecting entity definitions with data sources. You can examine the detailed setup in the `features.py` file located in the workspace/feature_repo folder.\n",
    "\n",
    "## Our Flight Delay Prediction Feature Views\n",
    "\n",
    "Our setup includes two feature views for flight delay prediction:\n",
    "\n",
    "### 1. Historical Flight Statistics\n",
    "\n",
    "```python\n",
    "flight_stats_fv = FeatureView(\n",
    "    name=\"flight_stats\",\n",
    "    entities=[flight],\n",
    "    ttl=timedelta(days=30),\n",
    "    schema=[\n",
    "        # Schema fields...\n",
    "    ],\n",
    "    online=True,\n",
    "    source=flight_stats_source,\n",
    "    tags={\"team\": \"flight_ops\"},\n",
    ")\n",
    "```\n",
    "\n",
    "This feature view:\n",
    "- Links to our `flight` entity\n",
    "- Sets a 30-day TTL for feature freshness\n",
    "- Defines a schema with flight attributes, delay metrics, and route information\n",
    "- Enables online serving (`online=True`)\n",
    "- Uses our batch file source for historical data\n",
    "- Includes team attribution tags\n",
    "\n",
    "### 2. Real-time Flight Statistics\n",
    "\n",
    "```python\n",
    "flight_stats_fresh_fv = FeatureView(\n",
    "    name=\"flight_stats_fresh\",\n",
    "    entities=[flight],\n",
    "    ttl=timedelta(days=30),\n",
    "    schema=[\n",
    "        # Real-time relevant fields...\n",
    "    ],\n",
    "    online=True,\n",
    "    source=flight_stats_push_source,\n",
    "    tags={\"team\": \"flight_ops\"},\n",
    ")\n",
    "```\n",
    "\n",
    "This feature view:\n",
    "- Connects to the same `flight` entity\n",
    "- Focuses on real-time features relevant for prediction\n",
    "- Uses our push source to enable real-time updates\n",
    "- Maintains the same TTL and online serving capabilities\n",
    "\n",
    "These feature views provide both historical data for model training and real-time data for online prediction, addressing the core requirements of our flight delay prediction service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb672713-b88f-477c-87bd-2d131309962b",
   "metadata": {},
   "source": [
    "# Feature Services in Feast\n",
    "\n",
    "## What are Feature Services?\n",
    "\n",
    "Feature Services are Feast components that allow you to group related features from one or more feature views into a logical unit. They provide a way to version your feature sets and create specific combinations of features for different model versions or use cases. You can examine the detailed setup in the `feature_services.py` file located in the `workspace/feature_repo` folder.\n",
    "\n",
    "## Our Flight Delay Prediction Feature Services\n",
    "\n",
    "Our project defines three feature services for different prediction scenarios:\n",
    "\n",
    "### 1. Basic Prediction (v1)\n",
    "\n",
    "```python\n",
    "flight_prediction_v1 = FeatureService(\n",
    "    name=\"flight_prediction_v1\",\n",
    "    features=[\n",
    "        flight_stats_fv[[\"Distance\", \"CRSElapsedTime\", \"DayOfWeek\", \"Month\"]],\n",
    "    ],\n",
    ")\n",
    "```\n",
    "\n",
    "This service:\n",
    "- Provides a minimal set of basic features for flight delay prediction\n",
    "- Selects only essential features from the main feature view\n",
    "- Suitable for baseline models or low-latency prediction requirements\n",
    "\n",
    "### 2. Advanced Prediction (v2)\n",
    "\n",
    "```python\n",
    "flight_prediction_v2 = FeatureService(\n",
    "    name=\"flight_prediction_v2\",\n",
    "    features=[\n",
    "        flight_stats_fv,\n",
    "    ],\n",
    ")\n",
    "```\n",
    "\n",
    "This service:\n",
    "- Includes all features from the historical flight statistics feature view\n",
    "- Provides the full feature set for comprehensive model training\n",
    "- Enables more sophisticated prediction models\n",
    "\n",
    "### 3. Real-time Prediction (v3)\n",
    "\n",
    "```python\n",
    "flight_prediction_v3 = FeatureService(\n",
    "    name=\"flight_prediction_v3\",\n",
    "    features=[flight_stats_fresh_fv],\n",
    ")\n",
    "```\n",
    "\n",
    "This service:\n",
    "- Uses only the real-time feature view with push source\n",
    "- Optimized for online serving with the most current data\n",
    "- Includes windowed features for real-time prediction scenarios\n",
    "\n",
    "By organizing our features into these services, we can easily manage different model versions, control which features are used in different scenarios, and maintain consistency across training and serving environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723428c-2042-4b28-8e7d-8f41444e86f4",
   "metadata": {},
   "source": [
    "# Setting Up the Infrastructure for Feast\n",
    "\n",
    "Before we can apply our feature definitions and start using Feast, we need to set up the required infrastructure for the offline, online store and registry. In our setup, we're using Redis for the online store (fast feature serving), minio for the offline store and PostgreSQL for the feature registry. \n",
    "\n",
    "You can start these services using Docker Compose with the following command in the SSH terminal:\n",
    "\n",
    "```bash\n",
    "docker-compose -f /home/cc/feast-artifact/docker/docker-compose-feast.yaml up redis registry minio minio-init\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bafba3-96d1-41cd-9a4a-2ac87eaf34b5",
   "metadata": {},
   "source": [
    "# Feast Apply\n",
    "\n",
    "`feast apply` is a command that deploys your feature definitions to a Feast feature store by:\n",
    "\n",
    "1. Scanning your repository for entity, project, feature view and service definitions [It will search all the .py files in the present working directory]\n",
    "2. Validating configurations \n",
    "3. Updating the registry metadata\n",
    "\n",
    "This command bridges your code definitions with the actual infrastructure, making your features available for both historical retrieval and online serving. For our flight delay service, running `feast apply` registers all components (entity, feature views, services) in the system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e00bb-4753-4b59-8301-98606e617fce",
   "metadata": {},
   "source": [
    "# Understanding feature_store.yaml in Feast\n",
    "\n",
    "The `feature_store.yaml` file is the central configuration file for your Feast deployment. It defines how your feature store connects to various infrastructure components and how it should behave. Let's break down each component in your configuration:\n",
    "\n",
    "```yaml\n",
    "project: flight_delay_project\n",
    "provider: local\n",
    "registry:\n",
    "  registry_type: sql\n",
    "  path: postgresql://postgres:mysecretpassword@registry:5432/feast\n",
    "online_store:\n",
    "  type: redis\n",
    "  connection_string: redis:6379\n",
    "offline_store:\n",
    "  type: file\n",
    "entity_key_serialization_version: 2\n",
    "```\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **project**: Defines the namespace for your feature store, serving as a logical grouping for all your feature definitions\n",
    "  \n",
    "- **provider**: Specifies the infrastructure provider (in this case \"local\" for development environments)\n",
    "  \n",
    "- **registry**: Configures how and where feature definitions are stored\n",
    "  - `registry_type: sql` - Uses a SQL database to store metadata\n",
    "  - `path` - Connection string for the PostgreSQL database\n",
    "\n",
    "- **online_store**: Defines the database for serving real-time feature values\n",
    "  - `type: redis` - Uses Redis for low-latency feature serving\n",
    "  - `connection_string` - How to connect to the Redis instance\n",
    "\n",
    "- **offline_store**: Specifies where historical feature values are stored\n",
    "  - `type: file` - Uses local files (like Parquet) for offline storage\n",
    "\n",
    "- **entity_key_serialization_version**: Controls how entity keys are serialized in the online store\n",
    "\n",
    "This configuration file is the first thing Feast looks for when you run commands like `feast apply` or when initializing a `FeatureStore` object. It establishes the connection between your code definitions and the actual infrastructure where your feature data will be stored and served.\n",
    "\n",
    "!! Docker Networking Note : We use container names (registry, redis) instead of localhost because services in Docker Compose networks can reference each other by container name and using localhost would make services connect to themselves, not to other services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5aa9d1-c0e2-4d32-a697-ba5e3c56f7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=509) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/workspace/feature_repo/entities.py:7: DeprecationWarning: Entity value_type will be mandatory in the next release. Please specify a value_type for entity 'flight'.\n",
      "  flight = Entity(\n",
      "No project found in the repository. Using project name flight_delay_project defined in feature_store.yaml\n",
      "Applying changes for project flight_delay_project\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mflight_stats_fresh\u001b[0m\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mflight_stats\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Please run the following cell to apply all the configurations that we've described in the above python files\n",
    "!feast apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3006d00-6e19-430d-b086-6a7a2c2fea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project: flight_delay_project\n",
      "provider: local\n",
      "registry:\n",
      "  registry_type: sql\n",
      "  path: postgresql+psycopg2://postgres:mysecretpassword@registry:5432/feast\n",
      "online_store:\n",
      "  type: redis\n",
      "  connection_string: redis:6379\n",
      "auth:\n",
      "  type: no_auth\n",
      "offline_store:\n",
      "  type: file\n",
      "batch_engine: local\n",
      "entity_key_serialization_version: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration present in feature_store.yaml\n",
    "!feast configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557aec5-b910-41ea-a8ce-a71f37ac65ef",
   "metadata": {},
   "source": [
    "The `FeatureStore(repo_path=\".\")` initializes a Feast feature store instance that connects to the feature repository in the current directory, providing access to all registered entities, features, and data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3fea5d-1eab-4f4e-9999-8beb9c432eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init feature store object\n",
    "store = FeatureStore(repo_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40781177-242d-4dbf-8dde-1482f0ab3e3e",
   "metadata": {},
   "source": [
    "# Patterns of feature retreival \n",
    "\n",
    "Feast supports several patterns of feature retrieval:\n",
    "\n",
    "1. Training data generation (via feature_store.get_historical_features(...))\n",
    "\n",
    "2. Offline feature retrieval for batch scoring (via feature_store.get_historical_features(...))\n",
    "\n",
    "3. Online feature retrieval for real-time model predictions (via feature_store.get_online_feature(...))\n",
    "\n",
    "In this tutorial, we will focus on training data generation and online feature retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c4ec5-41df-42cd-a681-9ac4eb6c7cd6",
   "metadata": {},
   "source": [
    "# Historical Feature Retrieval in Feast\n",
    "Historical feature retrieval is a critical component of the machine learning lifecycle that allows you to create point-in-time correct training datasets by joining features from your feature store with entity data. Let's explore how this works in Feast and why it's important for model training.\n",
    "\n",
    "## What is Historical Feature Retrieval?\n",
    "Historical feature retrieval in Feast allows you to:\n",
    "\n",
    "- Access historical feature values for a specific set of entities at precise timestamps\n",
    "- Create training datasets with point-in-time correct feature values\n",
    "- Prevent feature leakage by ensuring future data isn't used during training\n",
    "- Join features from multiple feature views into a single cohesive dataset\n",
    "\n",
    "Let's explore it using `fetch_historical_features_for_model_training` function below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "320bcf64-a490-4826-aeff-98cf121b5114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_historical_features_for_model_training(\n",
    "    store: FeatureStore, \n",
    "    feature_service_name: str = None,\n",
    "    start_date: datetime = None,\n",
    "    end_date: datetime = None,\n",
    "    flight_ids: list = None,\n",
    "    date_frequency: str = 'D' \n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve historical features for machine learning model training with point-in-time correctness.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    store : FeatureStore\n",
    "        The initialized Feast feature store\n",
    "    feature_service_name : str, optional\n",
    "        Name of the feature service to use for feature selection\n",
    "    start_date : datetime\n",
    "        Start date for the time range to retrieve features\n",
    "    end_date : datetime\n",
    "        End date for the time range to retrieve features\n",
    "    flight_ids : list, optional\n",
    "        List of flight_IDs to retrieve features for. If None, will use default example IDs\n",
    "    date_frequency : str, default='D'\n",
    "        Frequency for date range generation (D=daily, W=weekly, etc.)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing entity keys, timestamps, and corresponding feature values\n",
    "    \"\"\"\n",
    "    # Set defaults if not provided\n",
    "    if start_date is None:\n",
    "        start_date = datetime(2024, 9, 1)\n",
    "    if end_date is None:\n",
    "        end_date = datetime(2024, 9, 30)\n",
    "    if flight_ids is None:\n",
    "        flight_ids = [\"WN_3609\", \"WN_3610\", \"WN_3611\"]\n",
    "    \n",
    "    # Generate a range of dates between start_date and end_date\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=date_frequency)\n",
    "    \n",
    "    # Create entity dataframe with all combinations of flight_IDs and dates\n",
    "    entity_records = []\n",
    "    for flight_id in flight_ids:\n",
    "        for timestamp in date_range:\n",
    "            entity_records.append({\n",
    "                \"flight_ID\": flight_id,\n",
    "                \"event_timestamp\": timestamp\n",
    "            })\n",
    "    \n",
    "    entity_df = pd.DataFrame(entity_records)\n",
    "    \n",
    "    # Determine features to retrieve - either through feature service or direct references\n",
    "    if feature_service_name:\n",
    "        features_to_retrieve = store.get_feature_service(feature_service_name)\n",
    "        print(f\"Using feature service '{feature_service_name}' for model training\")\n",
    "    else:\n",
    "        # Default set of features if no feature service specified\n",
    "        features_to_retrieve = [\n",
    "            \"flight_stats:Distance\",\n",
    "            \"flight_stats:CRSElapsedTime\",\n",
    "            \"flight_stats:DayOfWeek\",\n",
    "            \"flight_stats:Month\",\n",
    "            \"flight_stats:WeatherDelay\",\n",
    "            \"flight_stats:NASDelay\",\n",
    "            \"flight_stats:is_holiday\",\n",
    "            \"flight_stats:days_to_nearest_holiday\",\n",
    "            \"flight_stats:route_avg_delay_24h\",\n",
    "        ]\n",
    "        print(f\"Using {len(features_to_retrieve)} individual feature references\")\n",
    "    \n",
    "    # Execute point-in-time join to retrieve historical features\n",
    "    print(f\"Retrieving historical features for {len(flight_ids)} flights across {len(date_range)} timestamps\")\n",
    "    historical_features = store.get_historical_features(\n",
    "        entity_df=entity_df,\n",
    "        features=features_to_retrieve\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame for model training\n",
    "    training_df = historical_features.to_df()\n",
    "    \n",
    "    print(f\"Successfully generated training dataset with {len(training_df)} rows and {len(training_df.columns)} columns\")\n",
    "    \n",
    "    # Can split into features and labels if needed for model training\n",
    "    # X = training_df.drop(['label_column'], axis=1) if 'label_column' exists\n",
    "    # y = training_df['label_column'] if 'label_column' exists\n",
    "    \n",
    "    return training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f43b352-eb9d-4132-a20a-6ff26f7fc8dd",
   "metadata": {},
   "source": [
    "# Historical Feature Retrieval for ML Model Training\n",
    "\n",
    "## Understanding Feast's Historical Feature Pipeline\n",
    "\n",
    "Historical feature retrieval is essential for building accurate, leakage-free machine learning models. Our implementation leverages Feast's powerful point-in-time joining capabilities to create production-ready training datasets.\n",
    "\n",
    "### Flexible Feature Selection\n",
    "\n",
    "Data scientists need to experiment with different feature combinations to optimize model performance. Our implementation supports:\n",
    "\n",
    "- **Feature Service Selection**: Use predefined feature groups via `store.get_feature_service()`\n",
    "- **Direct Feature References**: Specify individual features for more granular control\n",
    "- **Mixed Approach**: Combine feature services with additional individual features\n",
    "\n",
    "This flexibility accelerates the experimentation cycle while maintaining reproducibility.\n",
    "\n",
    "### Dataset Generation\n",
    "\n",
    "The output of `get_historical_features().to_df()` is immediately usable for model training\n",
    "\n",
    "###  ML Workflow Integration\n",
    "\n",
    "The resulting DataFrame integrates directly into standard ML pipelines:\n",
    "\n",
    "```python\n",
    "# Get training data with point-in-time correct features\n",
    "training_df = fetch_historical_features_for_model_training(\n",
    "    store,\n",
    "    feature_service_name=\"flight_prediction_v2\"\n",
    ")\n",
    "\n",
    "# Split into features and target\n",
    "X = training_df.drop(['ArrDelay'], axis=1)  # Features\n",
    "y = training_df['ArrDelay']                 # Prediction target\n",
    "\n",
    "# Standard scikit-learn workflow\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "This implementation bridges the gap between feature engineering and model development, allowing data scientists to focus on improving model performance while ensuring their training data maintains production-grade integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad0dbcc-1675-4095-8ea7-e927188412ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 individual feature references\n",
      "Retrieving historical features for 3 flights across 30 timestamps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/dask/dataframe/multi.py:169: UserWarning: Merging dataframes with merge column data type mismatches: \n",
      "+----------------------------+------------+-------------+\n",
      "| Merge columns              | left dtype | right dtype |\n",
      "+----------------------------+------------+-------------+\n",
      "| ('flight_ID', 'flight_ID') | object     | string      |\n",
      "+----------------------------+------------+-------------+\n",
      "Cast dtypes explicitly to avoid unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated training dataset with 89 rows and 11 columns\n",
      "Using feature service 'flight_prediction_v2' for model training\n",
      "Retrieving historical features for 3 flights across 30 timestamps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/dask/dataframe/multi.py:169: UserWarning: Merging dataframes with merge column data type mismatches: \n",
      "+----------------------------+------------+-------------+\n",
      "| Merge columns              | left dtype | right dtype |\n",
      "+----------------------------+------------+-------------+\n",
      "| ('flight_ID', 'flight_ID') | object     | string      |\n",
      "+----------------------------+------------+-------------+\n",
      "Cast dtypes explicitly to avoid unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated training dataset with 90 rows and 21 columns\n",
      "<bound method NDFrame.head of    flight_ID           event_timestamp  Distance  CRSElapsedTime  DayOfWeek  \\\n",
      "0    WN_3609 2024-09-01 00:00:00+00:00     619.0           105.0          7   \n",
      "1    WN_3610 2024-09-01 00:00:00+00:00    1670.0           250.0          7   \n",
      "2    WN_3610 2024-09-02 00:00:00+00:00     787.0           145.0          1   \n",
      "3    WN_3609 2024-09-02 00:00:00+00:00     888.0           155.0          1   \n",
      "4    WN_3609 2024-09-03 00:00:00+00:00     986.0           160.0          1   \n",
      "..       ...                       ...       ...             ...        ...   \n",
      "84   WN_3609 2024-09-29 00:00:00+00:00     392.0            80.0          7   \n",
      "85   WN_3610 2024-09-29 00:00:00+00:00     787.0           145.0          7   \n",
      "86   WN_3609 2024-09-30 00:00:00+00:00     986.0           160.0          1   \n",
      "87   WN_3610 2024-09-30 00:00:00+00:00     787.0           145.0          1   \n",
      "88   WN_3611 2024-09-30 00:00:00+00:00     879.0           150.0          1   \n",
      "\n",
      "    Month  WeatherDelay  NASDelay  is_holiday  days_to_nearest_holiday  \\\n",
      "0       9           NaN       NaN           0                        1   \n",
      "1       9           NaN       NaN           0                        1   \n",
      "2       9           NaN       NaN           1                        0   \n",
      "3       9           NaN       NaN           1                        0   \n",
      "4       9           NaN       NaN           1                        0   \n",
      "..    ...           ...       ...         ...                      ...   \n",
      "84      9           NaN       NaN           0                       15   \n",
      "85      9           0.0       8.0           0                       15   \n",
      "86      9           NaN       NaN           0                       14   \n",
      "87      9           NaN       NaN           0                       14   \n",
      "88      9           NaN       NaN           0                       14   \n",
      "\n",
      "    route_avg_delay_24h  \n",
      "0                   NaN  \n",
      "1                   NaN  \n",
      "2              3.583333  \n",
      "3             -2.555556  \n",
      "4             -7.500000  \n",
      "..                  ...  \n",
      "84            -8.000000  \n",
      "85            30.000000  \n",
      "86            -5.500000  \n",
      "87             6.538462  \n",
      "88            26.875000  \n",
      "\n",
      "[89 rows x 11 columns]>\n",
      "<bound method NDFrame.head of    flight_ID           event_timestamp Origin Dest  Distance  CRSElapsedTime  \\\n",
      "0    DL_2851 2024-09-01 00:00:00+00:00    SEA  DEN    1024.0           153.0   \n",
      "1    WN_3609 2024-09-01 00:00:00+00:00    ABQ  AUS     619.0           105.0   \n",
      "2    AA_1200 2024-09-01 00:00:00+00:00    MIA  PIT    1013.0           167.0   \n",
      "3    AA_1200 2024-09-02 00:00:00+00:00    MIA  PIT    1013.0           167.0   \n",
      "4    WN_3609 2024-09-02 00:00:00+00:00    ICT  LAS     986.0           160.0   \n",
      "..       ...                       ...    ...  ...       ...             ...   \n",
      "85   DL_2851 2024-09-29 00:00:00+00:00    SEA  DEN    1024.0           154.0   \n",
      "86   WN_3609 2024-09-29 00:00:00+00:00    STL  ICT     392.0            80.0   \n",
      "87   AA_1200 2024-09-30 00:00:00+00:00    MIA  PIT    1013.0           169.0   \n",
      "88   DL_2851 2024-09-30 00:00:00+00:00    SEA  DEN    1024.0           154.0   \n",
      "89   WN_3609 2024-09-30 00:00:00+00:00    ICT  LAS     986.0           160.0   \n",
      "\n",
      "    DayOfWeek  Month  Quarter  DepDelay  ...  WeatherDelay  NASDelay  \\\n",
      "0           7      9        3      -2.0  ...           NaN       NaN   \n",
      "1           7      9        3       2.0  ...           NaN       NaN   \n",
      "2           7      9        3      10.0  ...           NaN       NaN   \n",
      "3           1      9        3      -1.0  ...           NaN       NaN   \n",
      "4           1      9        3      -8.0  ...           NaN       NaN   \n",
      "..        ...    ...      ...       ...  ...           ...       ...   \n",
      "85          7      9        3      -8.0  ...           NaN       NaN   \n",
      "86          7      9        3      -6.0  ...           NaN       NaN   \n",
      "87          1      9        3      85.0  ...           0.0       0.0   \n",
      "88          1      9        3     -11.0  ...           NaN       NaN   \n",
      "89          1      9        3     -10.0  ...           NaN       NaN   \n",
      "\n",
      "    SecurityDelay  LateAircraftDelay  ArrDelay  is_holiday  \\\n",
      "0             NaN                NaN      -7.0           0   \n",
      "1             NaN                NaN      -4.0           0   \n",
      "2             NaN                NaN      10.0           0   \n",
      "3             NaN                NaN     -15.0           1   \n",
      "4             NaN                NaN     -21.0           1   \n",
      "..            ...                ...       ...         ...   \n",
      "85            NaN                NaN     -25.0           0   \n",
      "86            NaN                NaN     -19.0           0   \n",
      "87            0.0               16.0      68.0           0   \n",
      "88            NaN                NaN       0.0           0   \n",
      "89            NaN                NaN     -25.0           0   \n",
      "\n",
      "    days_to_nearest_holiday    Route route_avg_delay_24h  route_max_delay_24h  \n",
      "0                         1  SEA_DEN                 NaN                  NaN  \n",
      "1                         1  ABQ_AUS                 NaN                  NaN  \n",
      "2                         1  MIA_PIT                 NaN                  NaN  \n",
      "3                         0  MIA_PIT           59.500000                109.0  \n",
      "4                         0  ICT_LAS           -7.500000                 -7.0  \n",
      "..                      ...      ...                 ...                  ...  \n",
      "85                       15  SEA_DEN           12.210526                104.0  \n",
      "86                       15  STL_ICT           -8.000000                 -8.0  \n",
      "87                       14  MIA_PIT            5.000000                  5.0  \n",
      "88                       14  SEA_DEN            6.904762                 92.0  \n",
      "89                       14  ICT_LAS           -5.500000                 -1.0  \n",
      "\n",
      "[90 rows x 21 columns]>\n"
     ]
    }
   ],
   "source": [
    "basic_training_df = fetch_historical_features_for_model_training(store)\n",
    "\n",
    "# For advanced model with a specific feature service\n",
    "advanced_training_df = fetch_historical_features_for_model_training(\n",
    "    store,\n",
    "    feature_service_name=\"flight_prediction_v2\", \n",
    "    start_date=datetime(2024, 9, 1),\n",
    "    end_date=datetime(2024, 9, 30),\n",
    "    flight_ids=[\"DL_2851\", \"AA_1200\", \"WN_3609\"]\n",
    ")\n",
    "\n",
    "print(basic_training_df.head)\n",
    "\n",
    "print(advanced_training_df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d227b4-f107-4879-8ebd-3d0ffb9006f3",
   "metadata": {},
   "source": [
    "# Online Feature Retrieval with Feast\n",
    "\n",
    "## The `get_online_features()` Method\n",
    "\n",
    "Feast's `get_online_features()` method is designed for real-time feature retrieval by:\n",
    "1. Accepting entity rows (the keys we want to look up)\n",
    "2. Retrieving feature values from the online store \n",
    "3. Returning the values in a format ready for model inference\n",
    "\n",
    "## Our Implementation\n",
    "\n",
    "The function below demonstrates retrieving online features for specific flights in three different ways:\n",
    "1. Basic features directly from feature view\n",
    "2. Feature subsets via feature service (v1) for basic prediction using our file source\n",
    "3. All features via feature service (v2) for advanced prediction using our file source\n",
    "4. Real-time features via feature service (v3) for the freshest data using our push source\n",
    "\n",
    "This flexibility allows us to use the same underlying data in different prediction scenarios based on latency requirements and model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e197d82-8acc-46bf-a9e4-e34f0d8434c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_online_features(store, source: str = \"\"):\n",
    "    entity_rows = [\n",
    "        {\n",
    "            \"flight_ID\": \"OO_3757\",\n",
    "        },\n",
    "        {\n",
    "            \"flight_ID\": \"AA_1200\",\n",
    "        },\n",
    "        {\n",
    "            \"flight_ID\": \"DL_2851\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if source == \"feature_service\":\n",
    "        features_to_fetch = store.get_feature_service(\"flight_prediction_v1\")\n",
    "    elif source == \"advanced_feature_service\":\n",
    "        features_to_fetch = store.get_feature_service(\"flight_prediction_v2\")\n",
    "    elif source == \"real_time_feature_service\":\n",
    "        features_to_fetch = store.get_feature_service(\"flight_prediction_v3\")\n",
    "    else:\n",
    "        features_to_fetch = [\n",
    "            \"flight_stats:Distance\",\n",
    "            \"flight_stats:WeatherDelay\",\n",
    "        ]\n",
    "\n",
    "    returned_features = store.get_online_features(\n",
    "        features=features_to_fetch,\n",
    "        entity_rows=entity_rows,\n",
    "    ).to_dict()\n",
    "\n",
    "    for key, value in sorted(returned_features.items()):\n",
    "        print(key, \" : \", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c47c83-f3cf-4f36-963a-8ba2700d2b7f",
   "metadata": {},
   "source": [
    "# Materializing Features in Feast\n",
    "\n",
    "## What is Materialization?\n",
    "\n",
    "Materialization is the process of transferring feature values from offline storage (data source) to the online store (Redis in our case) to make them available for low-latency serving. This is a critical step that bridges the gap between your historical data and real-time feature access.\n",
    "\n",
    "## How Materialization Works\n",
    "\n",
    "The `materialize()` method:\n",
    "- Takes a time range (start_date to end_date)\n",
    "- Reads historical feature values from the data source for that period\n",
    "- Writes the latest values for each entity to the online store\n",
    "- Makes these features immediately available for online retrieval\n",
    "\n",
    "In our example, we're materializing all flight features for September 2024, ensuring that when we call `get_online_features()`, we'll have the most recent feature values for each flight entity ready for real-time prediction.\n",
    "\n",
    "This step is necessary before you can retrieve features with `get_online_features()` since it populates the online store with the data needed for low-latency lookups. Think of materialization as a scheduled ETL job that keeps your online feature store in sync with your offline data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcded951-cf17-40b3-b32c-0da1ab9e0a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views from \u001b[1m\u001b[32m2024-09-01 00:00:00+00:00\u001b[0m to \u001b[1m\u001b[32m2024-09-30 00:00:00+00:00\u001b[0m into the \u001b[1m\u001b[32mredis\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mflight_stats_fresh\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 21956/21956 [00:02<00:00, 9062.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mflight_stats\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 21956/21956 [00:02<00:00, 7453.75it/s]\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.strptime('2024-09-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2024-09-30', '%Y-%m-%d')\n",
    "\n",
    "store.materialize(start_date=start_date, end_date=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd4ff91e-ef1d-468c-b59d-080201787994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Online features retrieved through feature service v1 ---\n",
      "CRSElapsedTime  :  [129.0, 169.0, 154.0]\n",
      "DayOfWeek  :  [7, 7, 7]\n",
      "Distance  :  [604.0, 1013.0, 1024.0]\n",
      "Month  :  [9, 9, 9]\n",
      "flight_ID  :  ['OO_3757', 'AA_1200', 'DL_2851']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Online features retrieved through feature service v1 ---\")\n",
    "fetch_online_features(store, source=\"feature_service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ef97593-f1ad-440b-8070-c260e347c7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Online features retrieved through feature service v2 ---\n",
      "ArrDelay  :  [241.0, -7.0, -25.0]\n",
      "CRSElapsedTime  :  [129.0, 169.0, 154.0]\n",
      "CarrierDelay  :  [0.0, None, None]\n",
      "DayOfWeek  :  [7, 7, 7]\n",
      "DepDelay  :  [259.0, 5.0, -8.0]\n",
      "Dest  :  ['MSP', 'PIT', 'DEN']\n",
      "Distance  :  [604.0, 1013.0, 1024.0]\n",
      "LateAircraftDelay  :  [0.0, None, None]\n",
      "Month  :  [9, 9, 9]\n",
      "NASDelay  :  [0.0, None, None]\n",
      "Origin  :  ['SDF', 'MIA', 'SEA']\n",
      "Quarter  :  [3, 3, 3]\n",
      "Route  :  ['SDF_MSP', 'MIA_PIT', 'SEA_DEN']\n",
      "SecurityDelay  :  [0.0, None, None]\n",
      "WeatherDelay  :  [241.0, None, None]\n",
      "days_to_nearest_holiday  :  [15, 15, 15]\n",
      "flight_ID  :  ['OO_3757', 'AA_1200', 'DL_2851']\n",
      "is_holiday  :  [0, 0, 0]\n",
      "route_avg_delay_24h  :  [41.0, -5.0, 12.210526466369629]\n",
      "route_max_delay_24h  :  [80.0, -5.0, 104.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Online features retrieved through feature service v2 ---\")\n",
    "fetch_online_features(store, source=\"advanced_feature_service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "852ad54f-adf2-4e4d-b4cc-07b1dcc0841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Online features retrieved through feature service v3 ---\n",
      "CRSElapsedTime  :  [129.0, 169.0, 154.0]\n",
      "DayOfWeek  :  [7, 7, 7]\n",
      "DepDelay  :  [259.0, 5.0, -8.0]\n",
      "Dest  :  ['MSP', 'PIT', 'DEN']\n",
      "Distance  :  [604.0, 1013.0, 1024.0]\n",
      "Month  :  [9, 9, 9]\n",
      "NASDelay  :  [0.0, None, None]\n",
      "Origin  :  ['SDF', 'MIA', 'SEA']\n",
      "Quarter  :  [3, 3, 3]\n",
      "Route  :  ['SDF_MSP', 'MIA_PIT', 'SEA_DEN']\n",
      "WeatherDelay  :  [241.0, None, None]\n",
      "days_to_nearest_holiday  :  [15, 15, 15]\n",
      "flight_ID  :  ['OO_3757', 'AA_1200', 'DL_2851']\n",
      "is_holiday  :  [0, 0, 0]\n",
      "route_avg_delay_24h  :  [41.0, -5.0, 12.210526466369629]\n",
      "route_max_delay_24h  :  [80.0, -5.0, 104.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Online features retrieved through feature service v3 ---\")\n",
    "fetch_online_features(store, source=\"real_time_feature_service\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef9220f-e9df-4858-b2b7-6cfa86427ca3",
   "metadata": {},
   "source": [
    "# Real-Time Feature Updates with Push Source\n",
    "\n",
    "Feast's Push Source enables real-time updates to feature values without waiting for batch ingestion cycles. This capability is critical for ML applications where prediction accuracy depends on the most current data to avoid training-test skew.\n",
    "\n",
    "In the code below, we're demonstrating how to update feature values for three flight entities (`OO_3757`, `AA_1200`, and `DL_2851`) using the `store.push()` method. This approach:\n",
    "\n",
    "1. Creates a dummy DataFrame with the latest flight information\n",
    "3. Adds the mandatory `event_timestamp` field that Feast needs\n",
    "4. Pushes the data directly to the online store via our configured push source\n",
    "\n",
    "Unlike materialization (which loads historical data on a schedule), pushing allows immediate updates whenever new information becomes available. This is perfect for incorporating real-time signals like current weather conditions, airport congestion, or last-minute schedule changes into our delay prediction model.\n",
    "\n",
    "After pushing, these updated feature values are instantly available for retrieval through `get_online_features()`, ensuring our model makes predictions using the most current information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d000b41-d92c-40fc-879c-1de46cc5621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing dummy flight data to Feast...\n",
      "Push completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create dummy flight data for the three specified flights\n",
    "dummy_flights = [\n",
    "    {\n",
    "        \"flight_ID\": \"OO_3757\",\n",
    "        \"FlightDate\": \"2025-09-15\",\n",
    "        \"Origin\": \"SEA\",\n",
    "        \"Dest\": \"KTN\",\n",
    "        \"Distance\": 680.0,\n",
    "        \"CRSElapsedTime\": 140.0,\n",
    "        \"DayOfWeek\": 1,\n",
    "        \"Month\": 9,\n",
    "        \"Quarter\": 3,\n",
    "        \"DepDelay\": 5.0,\n",
    "        \"WeatherDelay\": 0.0,\n",
    "        \"NASDelay\": 2.0,\n",
    "        \"SecurityDelay\": 0.0,\n",
    "        \"LateAircraftDelay\": 0.0,\n",
    "        \"ArrDelay\": 10.0,\n",
    "        \"is_holiday\": 0,\n",
    "        \"days_to_nearest_holiday\": 17,  # Days to nearest holiday\n",
    "        \"Route\": \"SEA_KTN\",\n",
    "        \"route_avg_delay_24h\": 7.5,\n",
    "        \"route_max_delay_24h\": 15.0\n",
    "    },\n",
    "    {\n",
    "        \"flight_ID\": \"AA_1200\",\n",
    "        \"FlightDate\": \"2025-09-15\",\n",
    "        \"Origin\": \"MIA\",\n",
    "        \"Dest\": \"PIT\",\n",
    "        \"Distance\": 1013.0,\n",
    "        \"CRSElapsedTime\": 167.0,\n",
    "        \"DayOfWeek\": 1,\n",
    "        \"Month\": 9,\n",
    "        \"Quarter\": 3,\n",
    "        \"DepDelay\": 12.0,\n",
    "        \"WeatherDelay\": 5.0,\n",
    "        \"NASDelay\": 0.0,\n",
    "        \"SecurityDelay\": 0.0,\n",
    "        \"LateAircraftDelay\": 0.0,\n",
    "        \"ArrDelay\": 18.0,\n",
    "        \"is_holiday\": 0,\n",
    "        \"days_to_nearest_holiday\": 17,  # Days to nearest holiday\n",
    "        \"Route\": \"MIA_PIT\",\n",
    "        \"route_avg_delay_24h\": 22.5,\n",
    "        \"route_max_delay_24h\": 45.0\n",
    "    },\n",
    "    {\n",
    "        \"flight_ID\": \"DL_2851\",\n",
    "        \"FlightDate\": \"2025-09-15\",\n",
    "        \"Origin\": \"SEA\",\n",
    "        \"Dest\": \"DEN\",\n",
    "        \"Distance\": 1024.0,\n",
    "        \"CRSElapsedTime\": 153.0,\n",
    "        \"DayOfWeek\": 1,\n",
    "        \"Month\": 9,\n",
    "        \"Quarter\": 3,\n",
    "        \"DepDelay\": -3.0,  # Negative delay means early departure\n",
    "        \"WeatherDelay\": 0.0,\n",
    "        \"NASDelay\": 0.0,\n",
    "        \"SecurityDelay\": 0.0,\n",
    "        \"LateAircraftDelay\": 0.0,\n",
    "        \"ArrDelay\": -5.0,\n",
    "        \"is_holiday\": 0,\n",
    "        \"days_to_nearest_holiday\": 17,  # Days to nearest holiday\n",
    "        \"Route\": \"SEA_DEN\",\n",
    "        \"route_avg_delay_24h\": -2.5,\n",
    "        \"route_max_delay_24h\": 8.0\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "dummy_df = pd.DataFrame(dummy_flights)\n",
    "\n",
    "# Convert FlightDate to datetime for Feast\n",
    "dummy_df['FlightDate'] = pd.to_datetime(dummy_df['FlightDate'])\n",
    "dummy_df['event_timestamp'] = dummy_df['FlightDate']  # Feast requires 'event_timestamp' column\n",
    "\n",
    "# Push to Feast\n",
    "print(\"Pushing dummy flight data to Feast...\")\n",
    "store.push(\"flight_stats_push_source\", dummy_df)\n",
    "print(\"Push completed successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dec5ada9-6a3c-4a62-aa35-b4fdb29b19fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrDelay  :  [241.0, -7.0, -25.0]\n",
      "CRSElapsedTime  :  [129.0, 169.0, 154.0]\n",
      "CarrierDelay  :  [0.0, None, None]\n",
      "DayOfWeek  :  [7, 7, 7]\n",
      "DepDelay  :  [259.0, 5.0, -8.0]\n",
      "Dest  :  ['MSP', 'PIT', 'DEN']\n",
      "Distance  :  [604.0, 1013.0, 1024.0]\n",
      "LateAircraftDelay  :  [0.0, None, None]\n",
      "Month  :  [9, 9, 9]\n",
      "NASDelay  :  [0.0, None, None]\n",
      "Origin  :  ['SDF', 'MIA', 'SEA']\n",
      "Quarter  :  [3, 3, 3]\n",
      "Route  :  ['SDF_MSP', 'MIA_PIT', 'SEA_DEN']\n",
      "SecurityDelay  :  [0.0, None, None]\n",
      "WeatherDelay  :  [241.0, None, None]\n",
      "days_to_nearest_holiday  :  [15, 15, 15]\n",
      "flight_ID  :  ['OO_3757', 'AA_1200', 'DL_2851']\n",
      "is_holiday  :  [0, 0, 0]\n",
      "route_avg_delay_24h  :  [41.0, -5.0, 12.210526466369629]\n",
      "route_max_delay_24h  :  [80.0, -5.0, 104.0]\n"
     ]
    }
   ],
   "source": [
    "# Batch source feature service\n",
    "fetch_online_features(store, source=\"advanced_feature_service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e33be47e-82d3-4fc2-bf0b-5d8ce13c03bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRSElapsedTime  :  [140.0, 167.0, 153.0]\n",
      "DayOfWeek  :  [1, 1, 1]\n",
      "DepDelay  :  [5.0, 12.0, -3.0]\n",
      "Dest  :  ['KTN', 'PIT', 'DEN']\n",
      "Distance  :  [680.0, 1013.0, 1024.0]\n",
      "Month  :  [9, 9, 9]\n",
      "NASDelay  :  [2.0, 0.0, 0.0]\n",
      "Origin  :  ['SEA', 'MIA', 'SEA']\n",
      "Quarter  :  [3, 3, 3]\n",
      "Route  :  ['SEA_KTN', 'MIA_PIT', 'SEA_DEN']\n",
      "WeatherDelay  :  [0.0, 5.0, 0.0]\n",
      "days_to_nearest_holiday  :  [17, 17, 17]\n",
      "flight_ID  :  ['OO_3757', 'AA_1200', 'DL_2851']\n",
      "is_holiday  :  [0, 0, 0]\n",
      "route_avg_delay_24h  :  [7.5, 22.5, -2.5]\n",
      "route_max_delay_24h  :  [15.0, 45.0, 8.0]\n"
     ]
    }
   ],
   "source": [
    "# real time push source feature service\n",
    "fetch_online_features(store, source=\"real_time_feature_service\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146b70b-19ca-44ab-8821-9dae7b723960",
   "metadata": {},
   "source": [
    "Notice the difference where the feature service which has push source as its data source serves real time information whereas the other feature service serves historical data ? This is why push sources are important in feature stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899169c0-86bf-48bf-8962-d51eba43b719",
   "metadata": {},
   "source": [
    "# From Batch Processing to Streaming: Adding Real-Time Capabilities\n",
    "\n",
    "While Feast's push source provides a mechanism for updating features on-demand, it needs a source for this real time data. This is where streaming platforms like Redpanda (a Kafka-compatible streaming platform) become valuable. In the next section, we'll explore how to:\n",
    "\n",
    "1. Set up a streaming data pipeline that continuously processes flight information\n",
    "2. Apply transformations to incoming data to get computed features (adding holiday features and route statistics) \n",
    "3. Push the enriched data to our feature store in real-time\n",
    "\n",
    "This approach enables truly real-time ML predictions by maintaining feature freshness with minimal latency between data generation and availability. Unlike batch-based materialization which happens on a schedule, streaming allows our feature values to reflect the current state of the system at all times, making it ideal for time-sensitive applications like flight delay predictions where conditions change rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21517cd6-8b54-4af7-b7a2-8e0e5dfb2ddf",
   "metadata": {},
   "source": [
    "Now, lets bring up a popular streaming platform redpanda using docker-compose. Please execute the below command in SSH to bring redpanda up \n",
    "\n",
    "```bash\n",
    "docker-compose -f /home/cc/feast-artifact/docker/docker-compose-feast.yaml up redpanda console \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6aa87f-3b4b-4d9f-aad3-93c76fef4728",
   "metadata": {},
   "source": [
    "Here in the below cell, we initialize the core Kafka objects needed for our streaming data pipeline :\n",
    "\n",
    "- Connection to our Redpanda server (a Kafka-compatible streaming platform)\n",
    "- A producer client to send messages to the streaming platform\n",
    "- An admin client to manage topics and configurations\n",
    "\n",
    "Note : Redpanda is compatible with the Kafka Python library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41f04371-1e48-4335-8b97-ad1241c078c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "servers = 'redpanda:9092'\n",
    "producer = KafkaProducer(bootstrap_servers=servers)\n",
    "admin = KafkaAdminClient(bootstrap_servers=servers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e0293-78f7-4858-90f5-faf6680f135d",
   "metadata": {},
   "source": [
    "Run the below cell for the following : \n",
    "\n",
    "- Creates a new topic called \"test_topic\" in our streaming platform\n",
    "- Sets up 3 partitions to allow parallel processing of messages\n",
    "- Uses a replication factor of 1 (no redundancy, suitable for development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f095abe-b147-4220-acf1-2df0f7e4a17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='test_topic', error_code=0, error_message='Success')])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = NewTopic(name='test_topic', num_partitions=3, replication_factor=1)\n",
    "admin.create_topics([topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e015a29-28ca-4753-853d-e72c45d2f451",
   "metadata": {},
   "source": [
    "Now lets simulate real-time flight data stream by:\n",
    "\n",
    "- Reading our historical flight dataset\n",
    "- Updating the timestamps to make the data appear current\n",
    "- Publishing each flight record to our Redpanda topic\n",
    "\n",
    "Each iteration adds 52 weeks (1 year) to flight dates, creating the effect of fresh data flowing through our system. The short sleep between messages prevents overwhelming the consumer and simulates a more realistic data velocity.\n",
    "\n",
    "Navigate to http://localhost:8080/topics/test_topic in order to check the topic getting events\n",
    "\n",
    "Interrupt the kernel after sometime to stop the simulation ensuring we have enough data in the redpanda stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b161705-a8be-4797-a19f-10cffca0c5cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Send the row to Kafka\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     producer\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_topic\u001b[39m\u001b[38;5;124m'\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(row)\u001b[38;5;241m.\u001b[39mencode())\n\u001b[0;32m---> 22\u001b[0m     sleep(\u001b[38;5;241m0.2\u001b[39m) \n\u001b[1;32m     23\u001b[0m iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to simulate fresher data by updating timestamps\n",
    "def update_timestamps(row, iteration):\n",
    "    # Convert FlightDate to datetime\n",
    "    flight_date = pd.to_datetime(row[\"FlightDate\"])\n",
    "    # Add weeks to simulate fresher data\n",
    "    updated_flight_date = flight_date + pd.Timedelta(weeks=52 * iteration)\n",
    "    row[\"FlightDate\"] = updated_flight_date.strftime(\"%Y-%m-%d\")\n",
    "    return row\n",
    "\n",
    "iteration = 1\n",
    "while iteration<10:\n",
    "    for row in df[\n",
    "        ['flight_ID', 'FlightDate', 'Origin', 'Dest', 'Distance',\n",
    "       'CRSElapsedTime', 'DayOfWeek', 'Month', 'Quarter', 'DepDelay',\n",
    "       'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay',\n",
    "       'LateAircraftDelay', 'ArrDelay']\n",
    "        ].to_dict(\"records\"):\n",
    "        # Update timestamps to simulate fresher data\n",
    "        row = update_timestamps(row, iteration)\n",
    "        # Send the row to Kafka\n",
    "        producer.send('test_topic', json.dumps(row).encode())\n",
    "        sleep(0.2) \n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6d2f92-f439-4931-b46d-2efb8ac25152",
   "metadata": {},
   "source": [
    "# Setting up Kafka Consumer\n",
    "\n",
    "This code creates a consumer that will:\n",
    "\n",
    "- Subscribe to the 'test_topic' we created earlier\n",
    "- Connect to the same Redpanda server\n",
    "- Start reading from the earliest available message in the topic\n",
    "- Automatically deserialize the JSON messages back into Python dictionaries\n",
    "- Operate as part of the 'flight_feature_processor' consumer group (enabling multiple consumers to work together if needed)\n",
    "\n",
    "The consumer will pull flight data records from the stream so we can enrich them with additional features before pushing to our feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d0daea4-3b94-451e-ba10-39b22c71a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'test_topic',\n",
    "    bootstrap_servers=['redpanda:9092'],\n",
    "    auto_offset_reset='earliest',\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a8622-590b-4095-8966-367340d1df62",
   "metadata": {},
   "source": [
    "# Real-Time Stream Processing for Feature Engineering\n",
    "\n",
    "Our flight delay prediction service requires both historical and real-time features. While our batch data already contains pre-calculated features like route statistics and holiday information, our streaming data arrives in its raw form and needs real-time transformation.\n",
    "\n",
    "## Stream Processing Architecture\n",
    "\n",
    "This code implements a lightweight stream processing pipeline that:\n",
    "\n",
    "1. Consumes raw flight data from our Redpanda topic\n",
    "2. Applies stateful transformations to enrich the data with:\n",
    "   - Holiday-related features (is_holiday, days_to_nearest_holiday)\n",
    "   - Route-based statistics with 24-hour rolling windows (route_avg_delay_24h, route_max_delay_24h)\n",
    "3. Pushes the enriched features directly to our Feast feature store for immediate availability\n",
    "\n",
    "For production environments, this simple Python consumer could be replaced with more robust stream processing frameworks like:\n",
    "\n",
    "- **Apache Flink**: Would provide true stateful stream processing with exactly-once guarantees and advanced windowing operations, ideal for maintaining accurate route statistics over sliding time windows\n",
    "- **Apache Spark Structured Streaming**: Would enable batch and streaming unification with the same processing logic and integration with ML pipelines\n",
    "\n",
    "These frameworks would be particularly valuable for scenarios requiring:\n",
    "- Processing millions of flights in parallel\n",
    "- Complex windowing across multiple time dimensions\n",
    "- Integration with existing data pipelines\n",
    "- Scaling to handle holiday travel surges\n",
    "\n",
    "Our current implementation maintains state in memory using Python dictionaries, which works for demonstration but lacks the fault tolerance and scalability of dedicated stream processing frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30af3f1b-92b2-4ee4-99ce-7670058f9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function to add holiday features\n",
    "def add_holiday_features(flight_data):\n",
    "    \"\"\"Add holiday-related features to flight data.\"\"\"\n",
    "    # Convert FlightDate to datetime\n",
    "    flight_date = datetime.strptime(flight_data[\"FlightDate\"], \"%Y-%m-%d\")\n",
    "    \n",
    "    # Create US holiday calendar\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    # Get holidays for the year of flight\n",
    "    year = flight_date.year\n",
    "    holidays = cal.holidays(start=f'{year}-01-01', end=f'{year}-12-31')\n",
    "    \n",
    "    # Check if flight date is a holiday\n",
    "    flight_data[\"is_holiday\"] = 1 if flight_date.date() in holidays.date else 0\n",
    "    \n",
    "    # Calculate days to nearest holiday\n",
    "    days_diff = [abs((flight_date.date() - holiday).days) for holiday in holidays.date]\n",
    "    flight_data[\"days_to_nearest_holiday\"] = min(days_diff) if days_diff else 0\n",
    "    \n",
    "    return flight_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f90dd30-1dc7-4546-a98a-8820d30b89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize route delay storage for window calculations\n",
    "route_delays = {}  # Dictionary to track delays by route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3925c3c-5a5e-4ed0-9e49-671242397e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_route_statistics(flight_data):\n",
    "    \"\"\"Calculate rolling window statistics for routes.\"\"\"\n",
    "    # Create route key\n",
    "    origin = flight_data[\"Origin\"]\n",
    "    dest = flight_data[\"Dest\"]\n",
    "    route = f\"{origin}_{dest}\"\n",
    "    flight_data[\"Route\"] = route\n",
    "    \n",
    "    # Get flight date as datetime\n",
    "    flight_date = datetime.strptime(flight_data[\"FlightDate\"], \"%Y-%m-%d\")\n",
    "    \n",
    "    # Initialize route data structure if needed\n",
    "    if route not in route_delays:\n",
    "        route_delays[route] = []\n",
    "    \n",
    "    # Get departure delay (handle None values)\n",
    "    dep_delay = flight_data.get(\"DepDelay\")\n",
    "    if dep_delay is not None:\n",
    "        dep_delay = float(dep_delay)\n",
    "    else:\n",
    "        dep_delay = 0.0\n",
    "    \n",
    "    # Add current delay to history\n",
    "    route_delays[route].append((flight_date, dep_delay))\n",
    "    \n",
    "    # Keep only delays within 24 hour window\n",
    "    cutoff_time = flight_date - timedelta(hours=24)\n",
    "    route_delays[route] = [(ts, delay) for ts, delay in route_delays[route] \n",
    "                          if ts >= cutoff_time]\n",
    "    \n",
    "    # Calculate statistics if we have data\n",
    "    if route_delays[route]:\n",
    "        delays = [delay for _, delay in route_delays[route]]\n",
    "        flight_data[\"route_avg_delay_24h\"] = sum(delays) / len(delays)\n",
    "        flight_data[\"route_max_delay_24h\"] = max(delays)\n",
    "    else:\n",
    "        flight_data[\"route_avg_delay_24h\"] = 0\n",
    "        flight_data[\"route_max_delay_24h\"] = 0\n",
    "    \n",
    "    return flight_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6c33dfc-5e4f-4c87-9f28-32d4b74d612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_feast(flight_data, store):\n",
    "    \"\"\"Convert flight data to DataFrame and push to Feast.\"\"\"\n",
    "    # Create a DataFrame with a single row\n",
    "    df = pd.DataFrame([flight_data])\n",
    "    \n",
    "    # Convert FlightDate to datetime for Feast\n",
    "    df['FlightDate'] = pd.to_datetime(df['FlightDate'])\n",
    "    df['event_timestamp'] = df['FlightDate']\n",
    "    \n",
    "    try:\n",
    "        # Push to Feast\n",
    "        store.push(\"flight_stats_push_source\", df)\n",
    "        print(f\"Successfully pushed data for flight {flight_data['flight_ID']}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error pushing to Feast: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "024637ce-9f78-4275-8de2-91c6a807ef98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Kafka consumer. Processing flight data and pushing to Feast...\n",
      "Successfully pushed data for flight AS_502\n",
      "Processed flight AS_502 on route SEA_AUS (1 total)\n",
      "Successfully pushed data for flight AS_511\n",
      "Processed flight AS_511 on route STL_SEA (2 total)\n",
      "Successfully pushed data for flight AS_514\n",
      "Processed flight AS_514 on route RNO_SEA (3 total)\n",
      "Successfully pushed data for flight AS_515\n",
      "Processed flight AS_515 on route SEA_STL (4 total)\n",
      "Successfully pushed data for flight AS_499\n",
      "Processed flight AS_499 on route IAD_SEA (5 total)\n",
      "Successfully pushed data for flight AS_522\n",
      "Processed flight AS_522 on route SEA_BNA (6 total)\n",
      "Successfully pushed data for flight AS_531\n",
      "Processed flight AS_531 on route AUS_SAN (7 total)\n",
      "Successfully pushed data for flight AS_538\n",
      "Processed flight AS_538 on route SEA_ORD (8 total)\n",
      "Successfully pushed data for flight AS_517\n",
      "Processed flight AS_517 on route FLL_SEA (9 total)\n",
      "Successfully pushed data for flight AS_494\n",
      "Processed flight AS_494 on route SEA_SAT (10 total)\n",
      "Successfully pushed data for flight AS_443\n",
      "Processed flight AS_443 on route PHL_SEA (11 total)\n",
      "Successfully pushed data for flight AS_449\n",
      "Processed flight AS_449 on route MCI_SEA (12 total)\n",
      "Successfully pushed data for flight AS_450\n",
      "Processed flight AS_450 on route SEA_CMH (13 total)\n",
      "Successfully pushed data for flight AS_497\n",
      "Processed flight AS_497 on route ATL_SEA (14 total)\n",
      "Successfully pushed data for flight AS_460\n",
      "Processed flight AS_460 on route SEA_BOS (15 total)\n",
      "Successfully pushed data for flight AS_494\n",
      "Processed flight AS_494 on route SAT_SEA (16 total)\n",
      "Successfully pushed data for flight AS_442\n",
      "Processed flight AS_442 on route SEA_PHL (17 total)\n",
      "Successfully pushed data for flight AS_617\n",
      "Processed flight AS_617 on route DEN_SEA (18 total)\n",
      "Successfully pushed data for flight AS_620\n",
      "Processed flight AS_620 on route SEA_DEN (19 total)\n",
      "Successfully pushed data for flight AS_627\n",
      "Processed flight AS_627 on route SLC_SEA (20 total)\n",
      "Successfully pushed data for flight AS_632\n",
      "Processed flight AS_632 on route SEA_PHX (21 total)\n",
      "Successfully pushed data for flight AS_500\n",
      "Processed flight AS_500 on route SEA_IND (22 total)\n",
      "Successfully pushed data for flight AS_502\n",
      "Processed flight AS_502 on route SEA_AUS (23 total)\n",
      "Successfully pushed data for flight AS_511\n",
      "Processed flight AS_511 on route STL_SEA (24 total)\n",
      "Successfully pushed data for flight AS_512\n",
      "Processed flight AS_512 on route SFO_AUS (25 total)\n",
      "Successfully pushed data for flight AS_499\n",
      "Processed flight AS_499 on route IAD_SEA (26 total)\n",
      "Successfully pushed data for flight AS_522\n",
      "Processed flight AS_522 on route SEA_BNA (27 total)\n",
      "Successfully pushed data for flight AS_523\n",
      "Processed flight AS_523 on route RNO_SEA (28 total)\n",
      "Successfully pushed data for flight AS_527\n",
      "Processed flight AS_527 on route SFO_ANC (29 total)\n",
      "Successfully pushed data for flight AS_536\n",
      "Processed flight AS_536 on route SEA_BOS (30 total)\n",
      "Successfully pushed data for flight AS_537\n",
      "Processed flight AS_537 on route BOS_SEA (31 total)\n",
      "Successfully pushed data for flight AS_517\n",
      "Processed flight AS_517 on route FLL_SEA (32 total)\n",
      "Successfully pushed data for flight AS_543\n",
      "Processed flight AS_543 on route DFW_SEA (33 total)\n",
      "Successfully pushed data for flight AS_443\n",
      "Processed flight AS_443 on route PHL_SEA (34 total)\n",
      "Successfully pushed data for flight AS_451\n",
      "Processed flight AS_451 on route MSY_SEA (35 total)\n",
      "Successfully pushed data for flight AS_453\n",
      "Processed flight AS_453 on route IAH_SEA (36 total)\n",
      "Successfully pushed data for flight AS_459\n",
      "Processed flight AS_459 on route BOS_SEA (37 total)\n",
      "Successfully pushed data for flight AS_476\n",
      "Processed flight AS_476 on route SLC_SEA (38 total)\n",
      "Successfully pushed data for flight AS_485\n",
      "Processed flight AS_485 on route CHS_SEA (39 total)\n",
      "Successfully pushed data for flight AS_472\n",
      "Processed flight AS_472 on route SEA_STL (40 total)\n",
      "Successfully pushed data for flight AS_442\n",
      "Processed flight AS_442 on route SEA_PHL (41 total)\n",
      "Successfully pushed data for flight AS_618\n",
      "Processed flight AS_618 on route ABQ_SEA (42 total)\n",
      "Successfully pushed data for flight AS_619\n",
      "Processed flight AS_619 on route PHX_SEA (43 total)\n",
      "Successfully pushed data for flight AS_627\n",
      "Processed flight AS_627 on route SLC_SEA (44 total)\n",
      "Successfully pushed data for flight AS_629\n",
      "Processed flight AS_629 on route LAS_SEA (45 total)\n",
      "Successfully pushed data for flight AS_634\n",
      "Processed flight AS_634 on route SEA_SFO (46 total)\n",
      "Successfully pushed data for flight AS_640\n",
      "Processed flight AS_640 on route SEA_PHX (47 total)\n",
      "Successfully pushed data for flight AS_612\n",
      "Processed flight AS_612 on route SEA_LAS (48 total)\n",
      "Successfully pushed data for flight AS_575\n",
      "Processed flight AS_575 on route DFW_SEA (49 total)\n",
      "Successfully pushed data for flight AS_580\n",
      "Processed flight AS_580 on route PSP_SEA (50 total)\n",
      "Successfully pushed data for flight AS_595\n",
      "Processed flight AS_595 on route AUS_SEA (51 total)\n",
      "Successfully pushed data for flight AS_598\n",
      "Processed flight AS_598 on route SFO_ORD (52 total)\n",
      "Successfully pushed data for flight AS_600\n",
      "Processed flight AS_600 on route SEA_SMF (53 total)\n",
      "Successfully pushed data for flight AS_351\n",
      "Processed flight AS_351 on route IAD_SEA (54 total)\n",
      "Successfully pushed data for flight AS_352\n",
      "Processed flight AS_352 on route SEA_ORD (55 total)\n",
      "Successfully pushed data for flight AS_354\n",
      "Processed flight AS_354 on route SAN_ATL (56 total)\n",
      "Successfully pushed data for flight AS_356\n",
      "Processed flight AS_356 on route SFO_BOS (57 total)\n",
      "Successfully pushed data for flight AS_359\n",
      "Processed flight AS_359 on route TPA_SEA (58 total)\n",
      "Successfully pushed data for flight AS_361\n",
      "Processed flight AS_361 on route OMA_SEA (59 total)\n",
      "Successfully pushed data for flight AS_362\n",
      "Processed flight AS_362 on route SEA_IND (60 total)\n",
      "Successfully pushed data for flight AS_365\n",
      "Processed flight AS_365 on route DFW_SEA (61 total)\n",
      "Successfully pushed data for flight AS_369\n",
      "Processed flight AS_369 on route BNA_SEA (62 total)\n",
      "Successfully pushed data for flight AS_346\n",
      "Processed flight AS_346 on route SEA_MCO (63 total)\n",
      "Successfully pushed data for flight AS_318\n",
      "Processed flight AS_318 on route SEA_AUS (64 total)\n",
      "Successfully pushed data for flight AS_326\n",
      "Processed flight AS_326 on route SEA_TPA (65 total)\n",
      "Successfully pushed data for flight AS_329\n",
      "Processed flight AS_329 on route ORD_SEA (66 total)\n",
      "Successfully pushed data for flight AS_340\n",
      "Processed flight AS_340 on route PDX_ORD (67 total)\n",
      "Successfully pushed data for flight AS_438\n",
      "Processed flight AS_438 on route SEA_IAH (68 total)\n",
      "Successfully pushed data for flight AS_375\n",
      "Processed flight AS_375 on route LAS_SFO (69 total)\n",
      "Successfully pushed data for flight AS_410\n",
      "Processed flight AS_410 on route SEA_ORD (70 total)\n",
      "Successfully pushed data for flight AS_414\n",
      "Processed flight AS_414 on route SEA_DFW (71 total)\n",
      "Successfully pushed data for flight AS_415\n",
      "Processed flight AS_415 on route DFW_SEA (72 total)\n",
      "Successfully pushed data for flight AS_417\n",
      "Processed flight AS_417 on route SEA_CLE (73 total)\n",
      "Successfully pushed data for flight AS_420\n",
      "Processed flight AS_420 on route PDX_DEN (74 total)\n",
      "Successfully pushed data for flight AS_424\n",
      "Processed flight AS_424 on route SEA_BOS (75 total)\n",
      "Successfully pushed data for flight AS_424\n",
      "Processed flight AS_424 on route BOS_SEA (76 total)\n",
      "Successfully pushed data for flight AS_430\n",
      "Processed flight AS_430 on route SEA_MKE (77 total)\n",
      "Successfully pushed data for flight AS_433\n",
      "Processed flight AS_433 on route TPA_SAN (78 total)\n",
      "Successfully pushed data for flight AS_376\n",
      "Processed flight AS_376 on route SEA_ATL (79 total)\n",
      "Successfully pushed data for flight AS_404\n",
      "Processed flight AS_404 on route SEA_DTW (80 total)\n",
      "Successfully pushed data for flight AS_382\n",
      "Processed flight AS_382 on route SEA_BNA (81 total)\n",
      "Successfully pushed data for flight AS_500\n",
      "Processed flight AS_500 on route SEA_IND (82 total)\n",
      "Successfully pushed data for flight AS_505\n",
      "Processed flight AS_505 on route AUS_SEA (83 total)\n",
      "Successfully pushed data for flight AS_512\n",
      "Processed flight AS_512 on route SFO_AUS (84 total)\n",
      "Successfully pushed data for flight AS_518\n",
      "Processed flight AS_518 on route SEA_SLC (85 total)\n",
      "Successfully pushed data for flight AS_521\n",
      "Processed flight AS_521 on route PDX_BOS (86 total)\n",
      "Successfully pushed data for flight AS_523\n",
      "Processed flight AS_523 on route RNO_SEA (87 total)\n",
      "Successfully pushed data for flight AS_527\n",
      "Processed flight AS_527 on route SFO_ANC (88 total)\n",
      "Successfully pushed data for flight AS_534\n",
      "Processed flight AS_534 on route SFO_AUS (89 total)\n",
      "Successfully pushed data for flight AS_536\n",
      "Processed flight AS_536 on route SEA_BOS (90 total)\n",
      "Successfully pushed data for flight AS_498\n",
      "Processed flight AS_498 on route ATL_SEA (91 total)\n",
      "Successfully pushed data for flight AS_452\n",
      "Processed flight AS_452 on route SEA_IAH (92 total)\n",
      "Successfully pushed data for flight AS_477\n",
      "Processed flight AS_477 on route EWR_SEA (93 total)\n",
      "Successfully pushed data for flight AS_486\n",
      "Processed flight AS_486 on route SEA_ORD (94 total)\n",
      "Successfully pushed data for flight AS_490\n",
      "Processed flight AS_490 on route SEA_MSY (95 total)\n",
      "Successfully pushed data for flight AS_491\n",
      "Processed flight AS_491 on route RDU_SEA (96 total)\n",
      "Successfully pushed data for flight AS_544\n",
      "Processed flight AS_544 on route SEA_RDU (97 total)\n",
      "Successfully pushed data for flight AS_614\n",
      "Processed flight AS_614 on route SEA_SLC (98 total)\n",
      "Successfully pushed data for flight AS_618\n",
      "Processed flight AS_618 on route ABQ_SEA (99 total)\n",
      "Successfully pushed data for flight AS_619\n",
      "Processed flight AS_619 on route PHX_SEA (100 total)\n",
      "Successfully pushed data for flight AS_626\n",
      "Processed flight AS_626 on route HNL_SEA (101 total)\n",
      "Successfully pushed data for flight AS_628\n",
      "Processed flight AS_628 on route SEA_SLC (102 total)\n",
      "Successfully pushed data for flight AS_505\n",
      "Processed flight AS_505 on route AUS_SEA (103 total)\n",
      "Successfully pushed data for flight AS_510\n",
      "Processed flight AS_510 on route SLC_SEA (104 total)\n",
      "Successfully pushed data for flight AS_514\n",
      "Processed flight AS_514 on route RNO_SEA (105 total)\n",
      "Successfully pushed data for flight AS_515\n",
      "Processed flight AS_515 on route SEA_STL (106 total)\n",
      "Successfully pushed data for flight AS_516\n",
      "Processed flight AS_516 on route SEA_FLL (107 total)\n",
      "Successfully pushed data for flight AS_531\n",
      "Processed flight AS_531 on route AUS_SAN (108 total)\n",
      "Successfully pushed data for flight AS_498\n",
      "Processed flight AS_498 on route ATL_SEA (109 total)\n",
      "Successfully pushed data for flight AS_494\n",
      "Processed flight AS_494 on route SEA_SAT (110 total)\n",
      "Successfully pushed data for flight AS_446\n",
      "Processed flight AS_446 on route SEA_SAT (111 total)\n",
      "Successfully pushed data for flight AS_449\n",
      "Processed flight AS_449 on route MCI_SEA (112 total)\n",
      "Successfully pushed data for flight AS_452\n",
      "Processed flight AS_452 on route SEA_IAH (113 total)\n",
      "Successfully pushed data for flight AS_473\n",
      "Processed flight AS_473 on route STL_SEA (114 total)\n",
      "Successfully pushed data for flight AS_477\n",
      "Processed flight AS_477 on route EWR_SEA (115 total)\n",
      "Successfully pushed data for flight AS_486\n",
      "Processed flight AS_486 on route SEA_ORD (116 total)\n",
      "Successfully pushed data for flight AS_491\n",
      "Processed flight AS_491 on route RDU_SEA (117 total)\n",
      "Successfully pushed data for flight AS_494\n",
      "Processed flight AS_494 on route SAT_SEA (118 total)\n",
      "Successfully pushed data for flight AS_544\n",
      "Processed flight AS_544 on route SEA_RDU (119 total)\n",
      "Successfully pushed data for flight AS_555\n",
      "Processed flight AS_555 on route MSP_SEA (120 total)\n",
      "Successfully pushed data for flight AS_614\n",
      "Processed flight AS_614 on route SEA_SLC (121 total)\n",
      "Successfully pushed data for flight AS_617\n",
      "Processed flight AS_617 on route DEN_SEA (122 total)\n",
      "Successfully pushed data for flight AS_620\n",
      "Processed flight AS_620 on route SEA_DEN (123 total)\n",
      "Successfully pushed data for flight AS_624\n",
      "Processed flight AS_624 on route SEA_PHX (124 total)\n",
      "Successfully pushed data for flight AS_613\n",
      "Processed flight AS_613 on route LAS_SFO (125 total)\n",
      "Successfully pushed data for flight AS_632\n",
      "Processed flight AS_632 on route SEA_PHX (126 total)\n",
      "Successfully pushed data for flight AS_636\n",
      "Processed flight AS_636 on route SEA_LAS (127 total)\n",
      "Successfully pushed data for flight AS_639\n",
      "Processed flight AS_639 on route PHX_SEA (128 total)\n",
      "Successfully pushed data for flight AS_641\n",
      "Processed flight AS_641 on route TUS_SEA (129 total)\n",
      "Successfully pushed data for flight AS_642\n",
      "Processed flight AS_642 on route SEA_DEN (130 total)\n",
      "Successfully pushed data for flight AS_643\n",
      "Processed flight AS_643 on route DEN_SEA (131 total)\n",
      "Successfully pushed data for flight AS_644\n",
      "Processed flight AS_644 on route SEA_PHX (132 total)\n",
      "Successfully pushed data for flight AS_631\n",
      "Processed flight AS_631 on route SLC_SEA (133 total)\n",
      "Successfully pushed data for flight AS_555\n",
      "Processed flight AS_555 on route SEA_MSP (134 total)\n",
      "Successfully pushed data for flight AS_563\n",
      "Processed flight AS_563 on route MCI_SEA (135 total)\n",
      "Successfully pushed data for flight AS_564\n",
      "Processed flight AS_564 on route SEA_MCI (136 total)\n",
      "Successfully pushed data for flight AS_569\n",
      "Processed flight AS_569 on route MSY_SEA (137 total)\n",
      "Successfully pushed data for flight AS_572\n",
      "Processed flight AS_572 on route RDU_SEA (138 total)\n",
      "Successfully pushed data for flight AS_573\n",
      "Processed flight AS_573 on route DFW_SEA (139 total)\n",
      "Successfully pushed data for flight AS_576\n",
      "Processed flight AS_576 on route SEA_DFW (140 total)\n",
      "Successfully pushed data for flight AS_611\n",
      "Processed flight AS_611 on route PHX_SEA (141 total)\n",
      "Successfully pushed data for flight AS_586\n",
      "Processed flight AS_586 on route SEA_DFW (142 total)\n",
      "Successfully pushed data for flight AS_589\n",
      "Processed flight AS_589 on route DAL_SEA (143 total)\n",
      "Successfully pushed data for flight AS_594\n",
      "Processed flight AS_594 on route SEA_AUS (144 total)\n",
      "Successfully pushed data for flight AS_597\n",
      "Processed flight AS_597 on route ORD_SFO (145 total)\n",
      "Successfully pushed data for flight AS_601\n",
      "Processed flight AS_601 on route LAS_SEA (146 total)\n",
      "Successfully pushed data for flight AS_602\n",
      "Processed flight AS_602 on route PHX_SEA (147 total)\n",
      "Successfully pushed data for flight AS_608\n",
      "Processed flight AS_608 on route PDX_PHX (148 total)\n",
      "Successfully pushed data for flight AS_609\n",
      "Processed flight AS_609 on route LAS_SEA (149 total)\n",
      "Successfully pushed data for flight AS_437\n",
      "Processed flight AS_437 on route SAT_SEA (150 total)\n",
      "Successfully pushed data for flight AS_348\n",
      "Processed flight AS_348 on route SEA_IAD (151 total)\n",
      "Successfully pushed data for flight AS_357\n",
      "Processed flight AS_357 on route BOS_SFO (152 total)\n",
      "Successfully pushed data for flight AS_347\n",
      "Processed flight AS_347 on route MCO_SEA (153 total)\n",
      "Successfully pushed data for flight AS_364\n",
      "Processed flight AS_364 on route BOS_SEA (154 total)\n",
      "Successfully pushed data for flight AS_366\n",
      "Processed flight AS_366 on route SEA_MCO (155 total)\n",
      "Successfully pushed data for flight AS_370\n",
      "Processed flight AS_370 on route PHL_SEA (156 total)\n",
      "Successfully pushed data for flight AS_372\n",
      "Processed flight AS_372 on route SEA_DFW (157 total)\n",
      "Successfully pushed data for flight AS_373\n",
      "Processed flight AS_373 on route EWR_SFO (158 total)\n",
      "Successfully pushed data for flight AS_374\n",
      "Processed flight AS_374 on route SEA_MSP (159 total)\n",
      "Successfully pushed data for flight AS_344\n",
      "Processed flight AS_344 on route SEA_BWI (160 total)\n",
      "Successfully pushed data for flight AS_312\n",
      "Processed flight AS_312 on route SEA_OMA (161 total)\n",
      "Successfully pushed data for flight AS_313\n",
      "Processed flight AS_313 on route AUS_SEA (162 total)\n",
      "Successfully pushed data for flight AS_321\n",
      "Processed flight AS_321 on route ATL_SEA (163 total)\n",
      "Successfully pushed data for flight AS_323\n",
      "Processed flight AS_323 on route MKE_SEA (164 total)\n",
      "Successfully pushed data for flight AS_345\n",
      "Processed flight AS_345 on route CMH_SEA (165 total)\n",
      "Successfully pushed data for flight AS_332\n",
      "Processed flight AS_332 on route DTW_SEA (166 total)\n",
      "Successfully pushed data for flight AS_333\n",
      "Processed flight AS_333 on route ORD_SEA (167 total)\n",
      "Successfully pushed data for flight AS_335\n",
      "Processed flight AS_335 on route ORD_SEA (168 total)\n",
      "Successfully pushed data for flight AS_339\n",
      "Processed flight AS_339 on route SEA_LAS (169 total)\n",
      "Successfully pushed data for flight AS_328\n",
      "Processed flight AS_328 on route SEA_ORD (170 total)\n",
      "Successfully pushed data for flight AS_377\n",
      "Processed flight AS_377 on route ATL_SEA (171 total)\n",
      "Successfully pushed data for flight AS_406\n",
      "Processed flight AS_406 on route SEA_IAD (172 total)\n",
      "Successfully pushed data for flight AS_411\n",
      "Processed flight AS_411 on route BNA_SEA (173 total)\n",
      "Successfully pushed data for flight AS_412\n",
      "Processed flight AS_412 on route PIT_SEA (174 total)\n",
      "Successfully pushed data for flight AS_412\n",
      "Processed flight AS_412 on route SEA_PIT (175 total)\n",
      "Successfully pushed data for flight AS_419\n",
      "Processed flight AS_419 on route PDX_SAN (176 total)\n",
      "Successfully pushed data for flight AS_425\n",
      "Processed flight AS_425 on route SEA_SMF (177 total)\n",
      "Successfully pushed data for flight AS_426\n",
      "Processed flight AS_426 on route SMF_SEA (178 total)\n",
      "Successfully pushed data for flight AS_432\n",
      "Processed flight AS_432 on route SAN_TPA (179 total)\n",
      "Successfully pushed data for flight AS_421\n",
      "Processed flight AS_421 on route ORD_SEA (180 total)\n",
      "Successfully pushed data for flight AS_405\n",
      "Processed flight AS_405 on route OKC_SEA (181 total)\n",
      "Successfully pushed data for flight AS_379\n",
      "Processed flight AS_379 on route BWI_SEA (182 total)\n",
      "Successfully pushed data for flight AS_381\n",
      "Processed flight AS_381 on route BZN_SEA (183 total)\n",
      "Successfully pushed data for flight AS_383\n",
      "Processed flight AS_383 on route BNA_SEA (184 total)\n",
      "Successfully pushed data for flight AS_385\n",
      "Processed flight AS_385 on route MSP_SEA (185 total)\n",
      "Successfully pushed data for flight WN_3609\n",
      "Processed flight WN_3609 on route ABQ_AUS (186 total)\n",
      "Successfully pushed data for flight AS_508\n",
      "Processed flight AS_508 on route SEA_CVG (187 total)\n",
      "Successfully pushed data for flight AS_509\n",
      "Processed flight AS_509 on route ANC_SFO (188 total)\n",
      "Successfully pushed data for flight AS_510\n",
      "Processed flight AS_510 on route SLC_SEA (189 total)\n",
      "Successfully pushed data for flight AS_516\n",
      "Processed flight AS_516 on route SEA_FLL (190 total)\n",
      "Successfully pushed data for flight AS_537\n",
      "Processed flight AS_537 on route BOS_SEA (191 total)\n",
      "Successfully pushed data for flight AS_543\n",
      "Processed flight AS_543 on route DFW_SEA (192 total)\n",
      "Successfully pushed data for flight AS_445\n",
      "Processed flight AS_445 on route SEA_RNO (193 total)\n",
      "Successfully pushed data for flight AS_446\n",
      "Processed flight AS_446 on route SEA_SAT (194 total)\n",
      "Successfully pushed data for flight AS_447\n",
      "Processed flight AS_447 on route MKE_SEA (195 total)\n",
      "Successfully pushed data for flight AS_451\n",
      "Processed flight AS_451 on route MSY_SEA (196 total)\n",
      "Successfully pushed data for flight AS_453\n",
      "Processed flight AS_453 on route IAH_SEA (197 total)\n",
      "Successfully pushed data for flight AS_459\n",
      "Processed flight AS_459 on route BOS_SEA (198 total)\n",
      "Successfully pushed data for flight AS_473\n",
      "Processed flight AS_473 on route STL_SEA (199 total)\n",
      "Successfully pushed data for flight AS_476\n",
      "Processed flight AS_476 on route SLC_SEA (200 total)\n",
      "Successfully pushed data for flight AS_481\n",
      "Processed flight AS_481 on route MCO_SEA (201 total)\n",
      "Successfully pushed data for flight AS_485\n",
      "Processed flight AS_485 on route CHS_SEA (202 total)\n",
      "Successfully pushed data for flight AS_492\n",
      "Processed flight AS_492 on route SEA_ATL (203 total)\n",
      "Successfully pushed data for flight AS_472\n",
      "Processed flight AS_472 on route SEA_STL (204 total)\n",
      "Successfully pushed data for flight AS_555\n",
      "Processed flight AS_555 on route MSP_SEA (205 total)\n",
      "Successfully pushed data for flight AS_624\n",
      "Processed flight AS_624 on route SEA_PHX (206 total)\n",
      "Successfully pushed data for flight AS_629\n",
      "Processed flight AS_629 on route LAS_SEA (207 total)\n",
      "Successfully pushed data for flight AS_613\n",
      "Processed flight AS_613 on route LAS_SFO (208 total)\n",
      "Successfully pushed data for flight AS_630\n",
      "Processed flight AS_630 on route SEA_SLC (209 total)\n",
      "Successfully pushed data for flight WN_3609\n",
      "Processed flight WN_3609 on route ABQ_AUS (210 total)\n",
      "Successfully pushed data for flight AS_508\n",
      "Processed flight AS_508 on route SEA_CVG (211 total)\n",
      "Successfully pushed data for flight AS_509\n",
      "Processed flight AS_509 on route ANC_SFO (212 total)\n",
      "Successfully pushed data for flight AS_518\n",
      "Processed flight AS_518 on route SEA_SLC (213 total)\n",
      "Successfully pushed data for flight AS_521\n",
      "Processed flight AS_521 on route PDX_BOS (214 total)\n",
      "Successfully pushed data for flight AS_534\n",
      "Processed flight AS_534 on route SFO_AUS (215 total)\n",
      "Successfully pushed data for flight AS_538\n",
      "Processed flight AS_538 on route SEA_ORD (216 total)\n",
      "Successfully pushed data for flight AS_445\n",
      "Processed flight AS_445 on route SEA_RNO (217 total)\n",
      "Successfully pushed data for flight AS_447\n",
      "Processed flight AS_447 on route MKE_SEA (218 total)\n",
      "Successfully pushed data for flight AS_450\n",
      "Processed flight AS_450 on route SEA_CMH (219 total)\n",
      "Successfully pushed data for flight AS_497\n",
      "Processed flight AS_497 on route ATL_SEA (220 total)\n",
      "Successfully pushed data for flight AS_460\n",
      "Processed flight AS_460 on route SEA_BOS (221 total)\n",
      "Successfully pushed data for flight AS_481\n",
      "Processed flight AS_481 on route MCO_SEA (222 total)\n",
      "Successfully pushed data for flight AS_490\n",
      "Processed flight AS_490 on route SEA_MSY (223 total)\n",
      "Successfully pushed data for flight AS_492\n",
      "Processed flight AS_492 on route SEA_ATL (224 total)\n",
      "Successfully pushed data for flight AS_626\n",
      "Processed flight AS_626 on route HNL_SEA (225 total)\n",
      "Successfully pushed data for flight AS_628\n",
      "Processed flight AS_628 on route SEA_SLC (226 total)\n",
      "Successfully pushed data for flight AS_630\n",
      "Processed flight AS_630 on route SEA_SLC (227 total)\n",
      "Successfully pushed data for flight AS_645\n",
      "Processed flight AS_645 on route PHX_SEA (228 total)\n",
      "Successfully pushed data for flight AS_610\n",
      "Processed flight AS_610 on route PDX_LAS (229 total)\n",
      "Successfully pushed data for flight AS_568\n",
      "Processed flight AS_568 on route SEA_MSY (230 total)\n",
      "Successfully pushed data for flight AS_572\n",
      "Processed flight AS_572 on route SEA_RDU (231 total)\n",
      "Successfully pushed data for flight AS_588\n",
      "Processed flight AS_588 on route SEA_DAL (232 total)\n",
      "Successfully pushed data for flight AS_646\n",
      "Processed flight AS_646 on route SEA_PHX (233 total)\n",
      "Successfully pushed data for flight AS_439\n",
      "Processed flight AS_439 on route IAH_SEA (234 total)\n",
      "Successfully pushed data for flight AS_349\n",
      "Processed flight AS_349 on route BWI_SEA (235 total)\n",
      "Successfully pushed data for flight AS_353\n",
      "Processed flight AS_353 on route ORD_SEA (236 total)\n",
      "Successfully pushed data for flight AS_355\n",
      "Processed flight AS_355 on route ATL_SAN (237 total)\n",
      "Successfully pushed data for flight AS_358\n",
      "Processed flight AS_358 on route PDX_BOS (238 total)\n",
      "Successfully pushed data for flight AS_371\n",
      "Processed flight AS_371 on route SEA_PHL (239 total)\n",
      "Successfully pushed data for flight AS_360\n",
      "Processed flight AS_360 on route SEA_TPA (240 total)\n",
      "Successfully pushed data for flight AS_310\n",
      "Processed flight AS_310 on route LAX_IAD (241 total)\n",
      "Successfully pushed data for flight AS_311\n",
      "Processed flight AS_311 on route BOS_PDX (242 total)\n",
      "Successfully pushed data for flight AS_315\n",
      "Processed flight AS_315 on route CLE_SEA (243 total)\n",
      "Successfully pushed data for flight AS_319\n",
      "Processed flight AS_319 on route SEA_ANC (244 total)\n",
      "Successfully pushed data for flight AS_327\n",
      "Processed flight AS_327 on route SEA_ATL (245 total)\n",
      "Successfully pushed data for flight AS_334\n",
      "Processed flight AS_334 on route SEA_ATL (246 total)\n",
      "Successfully pushed data for flight AS_338\n",
      "Processed flight AS_338 on route SEA_IAD (247 total)\n",
      "Successfully pushed data for flight AS_342\n",
      "Processed flight AS_342 on route SAN_AUS (248 total)\n",
      "Successfully pushed data for flight AS_343\n",
      "Processed flight AS_343 on route AUS_SAN (249 total)\n",
      "Successfully pushed data for flight AS_408\n",
      "Processed flight AS_408 on route SEA_ORD (250 total)\n",
      "Successfully pushed data for flight AS_406\n",
      "Processed flight AS_406 on route IAD_SEA (251 total)\n",
      "Successfully pushed data for flight AS_422\n",
      "Processed flight AS_422 on route SEA_IAH (252 total)\n",
      "Successfully pushed data for flight AS_427\n",
      "Processed flight AS_427 on route SEA_DEN (253 total)\n",
      "Successfully pushed data for flight AS_428\n",
      "Processed flight AS_428 on route SEA_DEN (254 total)\n",
      "Successfully pushed data for flight AS_378\n",
      "Processed flight AS_378 on route SEA_BWI (255 total)\n",
      "Successfully pushed data for flight AS_384\n",
      "Processed flight AS_384 on route SFO_EWR (256 total)\n",
      "No messages received. Empty poll count: 1/3\n",
      "No messages received. Empty poll count: 2/3\n",
      "No messages received. Empty poll count: 3/3\n",
      "Finished processing 256 messages after 3 empty polls\n",
      "Kafka consumer closed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Kafka consumer. Processing flight data and pushing to Feast...\")\n",
    "try:\n",
    "    # Set a timeout for polling messages\n",
    "    message_count = 0\n",
    "    max_empty_polls = 3\n",
    "    empty_poll_count = 0\n",
    "    \n",
    "    # Continue polling until we reach max_empty_polls consecutive empty polls\n",
    "    while empty_poll_count < max_empty_polls:\n",
    "        # Poll with a timeout (e.g., 5 seconds)\n",
    "        messages = consumer.poll(timeout_ms=5)\n",
    "        \n",
    "        if not messages:\n",
    "            print(f\"No messages received. Empty poll count: {empty_poll_count + 1}/{max_empty_polls}\")\n",
    "            empty_poll_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Reset empty poll count if we got messages\n",
    "        empty_poll_count = 0\n",
    "        \n",
    "        # Process all messages received in this poll\n",
    "        for topic_partition, partition_messages in messages.items():\n",
    "            for message in partition_messages:\n",
    "                # Get the flight data from Kafka\n",
    "                flight_data = message.value\n",
    "                \n",
    "                # 1. Add holiday features\n",
    "                flight_data = add_holiday_features(flight_data)\n",
    "                \n",
    "                # 2. Add route statistics\n",
    "                flight_data = update_route_statistics(flight_data)\n",
    "                \n",
    "                # 3. Push to Feast\n",
    "                store = FeatureStore(repo_path=\".\")\n",
    "                push_to_feast(flight_data, store)\n",
    "                \n",
    "                # Count and print update\n",
    "                message_count += 1\n",
    "                print(f\"Processed flight {flight_data['flight_ID']} on route {flight_data['Route']} ({message_count} total)\")\n",
    "    \n",
    "    print(f\"Finished processing {message_count} messages after {max_empty_polls} empty polls\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Consumer stopped by user.\")\n",
    "finally:\n",
    "    consumer.close()\n",
    "    print(\"Kafka consumer closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331027d7-8fe1-4ae1-bc9f-34cfef249857",
   "metadata": {},
   "source": [
    "# Airflow Integration\n",
    "To ensure fresh features, you'll want to schedule materialization jobs regularly. This can be as simple as having a cron job that calls feast materialize-incremental.\n",
    "\n",
    "Airflow can be an important tool in scheduling this. Let's bring up airflow by executing the following command in SSH:\n",
    "```bash\n",
    "docker-compose -f /home/cc/feast-artifact/docker/docker-compose-airflow.yml up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b21d95-03ac-413d-8da0-682a03c20318",
   "metadata": {},
   "source": [
    "In a browser, open `http://localhost:8081`. Log in with username airflow@example.com and password airflow (we have created an initial user with these credentials in our Docker compose file).\n",
    "\n",
    "We have already seen how DAG can be used to automate workloops in the Feedback loop tutorial. In the same way, we have created a DAG that automates materialization using `dag.py`\n",
    "\n",
    "```python\n",
    "import os\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime, timedelta\n",
    "from feast import RepoConfig, FeatureStore\n",
    "import pendulum\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dag(\n",
    "    schedule=\"@hourly\",  # Adjust the schedule as needed\n",
    "    catchup=False,\n",
    "    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n",
    "    tags=[\"feast\"],\n",
    ")\n",
    "def materialize_dag():\n",
    "    @task()\n",
    "    def materialize():\n",
    "        repo_path = '/opt/airflow/feature_repo/'\n",
    "        \n",
    "        # Print current working directory for debugging\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        print(f\"Using feature repository at: {repo_path}\")\n",
    "        \n",
    "        # Use FeatureStore with explicit repo_path\n",
    "        store = FeatureStore(repo_path=repo_path)\n",
    "        \n",
    "        # Calculate start and end time for materialization\n",
    "        end_time = datetime.utcnow()\n",
    "        start_time = end_time - timedelta(hours=1)\n",
    "        \n",
    "        print(f\"Materializing data from {start_time} to {end_time}\")\n",
    "        \n",
    "        # Materialize the previous hour of data\n",
    "        store.materialize(start_time, end_time)\n",
    "\n",
    "    materialize()\n",
    "\n",
    "\n",
    "# This line is required for Airflow to properly register the DAG\n",
    "materialize_dag = materialize_dag()\n",
    "```\n",
    "\n",
    "Let's break this down :\n",
    "\n",
    "1. The `@dag` decorator defines basic DAG properties:\n",
    "   - `schedule=\"@hourly\"` - Runs every hour\n",
    "   - `catchup=False` - Doesn't run for missed intervals\n",
    "   - `start_date` - When the DAG becomes active\n",
    "   - `tags=[\"feast\"]` - For organization in the UI\n",
    "\n",
    "2. Inside the DAG, there's a single `@task` called `materialize()` that:\n",
    "   - Sets the path to your feature repository\n",
    "   - Creates a FeatureStore instance\n",
    "   - Calculates a time window (the previous hour)\n",
    "   - Calls the `materialize()` method to update offline feature values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f36e7-c13c-4274-b435-8e1e5a409bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02687012-ca74-4dce-bafa-49d29f1070c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52eca5-f8e7-461a-9bd7-1be3acfb55d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcec6ae-adf2-4bfc-86d0-417d570d1c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d99d70-5a86-4783-ac17-1355e30a9d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
